{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tp3 : NLP Classification de Texte"
      ],
      "metadata": {
        "id": "sgWc5I3q35T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "nIZyJywFYStD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d4bb7e-2a33-43fd-b3c5-4e051ecf5aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le dataset\n",
        "df = pd.read_csv(\"movie_review.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImxDCZUtYWpX",
        "outputId": "39efd763-914c-4117-c200-46ea1144417c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   fold_id cv_tag  html_id  sent_id  \\\n",
            "0        0  cv000    29590        0   \n",
            "1        0  cv000    29590        1   \n",
            "2        0  cv000    29590        2   \n",
            "3        0  cv000    29590        3   \n",
            "4        0  cv000    29590        4   \n",
            "\n",
            "                                                text  tag  \n",
            "0  films adapted from comic books have had plenty...  pos  \n",
            "1  for starters , it was created by alan moore ( ...  pos  \n",
            "2  to say moore and campbell thoroughly researche...  pos  \n",
            "3  the book ( or \" graphic novel , \" if you will ...  pos  \n",
            "4  in other words , don't dismiss this film becau...  pos  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pré-traitement (Pre-processing )des données textuelles\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Supprimer la ponctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "    # Supprimer les stop words\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(df['processed_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0au__4IY-sD",
        "outputId": "3b79a604-7a0d-4222-c445-8256a7986dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        films adapted comic books plenty success wheth...\n",
            "1        starters created alan moore eddie campbell bro...\n",
            "2        say moore campbell thoroughly researched subje...\n",
            "3        book graphic novel 500 pages long includes nea...\n",
            "4                           words dont dismiss film source\n",
            "                               ...                        \n",
            "64715      lack inspiration traced back insipid characters\n",
            "64716    like many skits current incarnation saturdayni...\n",
            "64717    watching one roxbury skits snl come away chara...\n",
            "64718                        bump unsuspecting women thats\n",
            "64719       watching anightattheroxbury youll left exactly\n",
            "Name: processed_text, Length: 64720, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Entraînement du modèle Word2Vec\n",
        "tokenized_text = [word_tokenize(text) for text in df['processed_text']]\n",
        "#La tokenization divise chaque texte de la colonne processed_text en une liste de mots (ou tokens).\n",
        "word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Afficher le vecteur pour un mot spécifique (par exemple, 'movie')\n",
        "word_vector = word2vec_model.wv['movie']\n",
        "print(\"Vecteur pour le mot 'movie':\", word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDMOQs-rdblM",
        "outputId": "f1fd5973-8f26-46ff-8295-f2ff6bd0e1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vecteur pour le mot 'movie': [-0.7021366   0.568045    0.7448934   0.67349255  0.18114206 -1.0586964\n",
            "  2.0526474   1.5601648  -2.1281545  -0.43298176 -0.5941331  -1.2646284\n",
            "  0.77901334  0.779618    0.04520872 -0.9423847   0.8527799  -0.49041417\n",
            " -0.7018608  -1.9959956   0.6741901  -0.22534531  0.62059826 -1.323488\n",
            " -0.44091684  0.16065687 -0.97726685 -0.06948443 -1.2435945  -0.01808226\n",
            "  0.8546094  -0.08810988  0.90132314 -0.20489883 -0.39073825  1.1316277\n",
            "  0.6912175   0.20495531 -0.14154923 -1.8532808   0.19506171 -0.27355957\n",
            " -0.71059585  0.63829     0.44012824 -0.18142009 -0.5368987  -0.29997614\n",
            "  0.612479    0.47255173 -0.2966137  -0.15265651 -0.81501895  0.01716703\n",
            " -0.0318241  -0.22544399  0.8215619  -0.6543784  -0.8585152   0.92699295\n",
            "  0.00499394 -0.36601704  0.06998275 -0.5575904  -1.5195374   0.94812256\n",
            "  0.50012547  0.6007947  -1.6962597   1.2874672  -1.2753365   0.30237907\n",
            "  1.0327144  -0.22284757  0.47078875 -0.5712742   0.5600681   0.36441335\n",
            " -0.6375482   0.25943977 -0.8947756  -1.0234978  -0.1877313   0.8504797\n",
            " -0.21820478 -0.29083192  0.39858028  0.5953964   1.034633    0.7817989\n",
            "  0.26859158  0.07120972  0.83527577 -0.17192686  2.3019528   0.12075894\n",
            " -0.2866724  -0.6253782   1.0437909   0.32948557]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorisation des reviews de movies\n",
        "def vectorize_text(text, model):\n",
        "    words = word_tokenize(text)\n",
        "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    if vectors:\n",
        "        return sum(vectors) / len(vectors)\n",
        "    else:\n",
        "        return [0] * model.vector_size\n",
        "\n",
        "df['vectorized_text'] = df['processed_text'].apply(lambda x: vectorize_text(x, word2vec_model))\n",
        "print(df['vectorized_text'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80uaWan8esOG",
        "outputId": "a3cd745b-2b96-40a2-b708-afac877b58c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [-0.24738699, 0.44466636, 0.22824238, 0.09249,...\n",
            "1    [-0.184112, 0.29823, 0.11517828, 0.11264722, 0...\n",
            "2    [-0.2228232, 0.45173407, 0.23681158, 0.0067303...\n",
            "3    [-0.1954853, 0.2628056, 0.1539347, 0.1190092, ...\n",
            "4    [-0.2531006, 0.36936074, 0.3380906, 0.18004978...\n",
            "Name: vectorized_text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF1MWJnMPxCs"
      },
      "outputs": [],
      "source": [
        "# Division des données\n",
        "X = pd.DataFrame(df['vectorized_text'].to_list())\n",
        "y = df['tag']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#print(\"Ensemble d'entraînement X_train :\\n\", X_train.head())\n",
        "#print(\"\\nEnsemble de test X_test :\\n\", X_test.head())\n",
        "#print(\"\\nEnsemble d'entraînement y_train :\\n\", y_train.head())\n",
        "#print(\"\\nEnsemble de test y_test :\\n\", y_test.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Construction d'un classificateur\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "AzzSGBEdfNZg",
        "outputId": "9696e847-98e4-440e-c6f0-2e46b2814546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation du modèle\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5luuZdZSfGFn",
        "outputId": "e06abbe6-1f19-4dfb-c01e-9617c3fe6493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5682941903584673\n",
            "Precision: 0.5698996581019949\n",
            "Recall: 0.5682941903584673\n",
            "F1 score: 0.5629087775036085\n"
          ]
        }
      ]
    }
  ]
}